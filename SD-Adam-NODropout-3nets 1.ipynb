{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the experiments - training with Adam and no dropout\n",
    "### Importing and preparing data\n",
    "First we import the relevant packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset MNIST_fashion. It is an MNIST-like dataset, but contains images of clothing instead of numbers. The image size is the same as in MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the pixels to values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are given in integer values between 0 and 9. For the Self-Distillation we convert these into the one-hot representation, for example a trouser with label [1] now has label [0,1,0,0,0,0,0,0,0,0] with 1 at the first entry (note that counting starts with 0 here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_onehot = to_categorical(train_labels)\n",
    "test_labels_onehot = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the data so that it fits into the functions later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rows=28\n",
    "im_cols=28\n",
    "im_shape=(im_rows,im_cols,1)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0],*im_shape)\n",
    "test_images = test_images.reshape(test_images.shape[0],*im_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models\n",
    "Defining all the models that will be used later, so that we can see a summary. Note that the each model will be re-initialized later in each for-step, since each Self-Distillation step requires a fresh model that hasn't been trained yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Convolutional_2_Layer'\n",
    "cnn_model_2 = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation ='softmax')], name = name)\n",
    "\n",
    "name = 'Convolutional_4_Layer_CE'\n",
    "cnn_model_4 = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Conv2D(filters = 128, kernel_size =3, activation ='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10)], name = name)\n",
    "\n",
    "name = 'Feedforward_2_Layer'\n",
    "ff_model_2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=im_shape),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10, activation ='softmax')], name = name)\n",
    "\n",
    "\n",
    "models = [cnn_model_2, cnn_model_4, ff_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Convolutional_2_Layer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                495680    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 515,146\n",
      "Trainable params: 515,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Convolutional_4_Layer_CE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,217,962\n",
      "Trainable params: 1,217,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Feedforward_2_Layer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN's with cross-entropy loss\n",
    "##### Training the 2-layer CNN with cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labels_Iterations_2layerCE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_2layerCE = train_labels_onehot\n",
    "predictions_2layerCE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_2CNNCE = np.zeros(8,dtype='float32')\n",
    "test_acc_2CNNCE = np.zeros(8,dtype='float32')\n",
    "for i in range(8):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Convolutional_2_Layer_CE'\n",
    "    cnn_model_2CE = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)], name = name)\n",
    "    \n",
    "    cnn_model_2CE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    cnn_model_2CE.fit(train_images, label_update_2layerCE, epochs=10)\n",
    "    probability_cnn_model_2CE = tf.keras.Sequential([cnn_model_2CE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_2CNNCE[i], test_acc_2CNNCE[i] = cnn_model_2CE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_2CNNCE[i])\n",
    "    label_update_2layerCE = probability_cnn_model_2CE.predict(train_images)\n",
    "    predictions_2layerCE[:,:,i] = probability_cnn_model_2CE.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the 4layer convoluiontal NN with cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_Iterations_4layerCE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_4layerCE = train_labels_onehot\n",
    "predictions_4layerCE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_4CNNCE = np.zeros(8,dtype='float32')\n",
    "test_acc_4CNNCE = np.zeros(8,dtype='float32')\n",
    "for i in range(8):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Convolutional_4_Layer_CE'\n",
    "    cnn_model_4CE = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Conv2D(filters = 128, kernel_size =3, activation ='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10)], name = name)\n",
    "    \n",
    "    cnn_model_4CE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    cnn_model_4CE.fit(train_images, label_update_4layerCE, epochs=10)\n",
    "    probability_cnn_model_4CE = tf.keras.Sequential([cnn_model_4CE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_4CNNCE[i], test_acc_4CNNCE[i] = cnn_model_4CE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_4CNNCE[i])\n",
    "    label_update_4layerCE = probability_cnn_model_4CE.predict(train_images)\n",
    "    predictions_4layerCE[:,:,i] = probability_cnn_model_4CE.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN's with mean-squared-error loss\n",
    "##### Training the 2-layer CNN mean-squared-error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_Iterations_2layerMSE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_2layerMSE = train_labels_onehot\n",
    "predictions_2layerMSE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_2CNNMSE = np.zeros(8,dtype='float32')\n",
    "test_acc_2CNNMSE = np.zeros(8,dtype='float32')\n",
    "for i in range(8):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Convolutional_2_Layer_MSE'\n",
    "    cnn_model_2MSE = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10)], name = name)\n",
    "    \n",
    "    cnn_model_2MSE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    cnn_model_2MSE.fit(train_images, label_update_2layerMSE, epochs=10)\n",
    "    probability_cnn_model_2MSE = tf.keras.Sequential([cnn_model_2MSE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_2CNNMSE[i], test_acc_2CNNMSE[i] = cnn_model_2MSE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_2CNNMSE[i])\n",
    "    label_update_2layerMSE = probability_cnn_model_2MSE.predict(train_images)\n",
    "    predictions_2layerMSE[:,:,i] = probability_cnn_model_2MSE.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the 4layer convoluiontal NN with mean-squared-error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_Iterations_4layerMSE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_4layerMSE = train_labels_onehot\n",
    "predictions_4layerMSE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_4CNNMSE = np.zeros(8,dtype='float32')\n",
    "test_acc_4CNNMSE = np.zeros(8,dtype='float32')\n",
    "for i in range(18):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Convolutional_4_Layer_MSE'\n",
    "    cnn_model_4MSE = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,input_shape=im_shape),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters = 64, kernel_size = 3, activation ='relu'),\n",
    "    keras.layers.Conv2D(filters = 128, kernel_size =3, activation ='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10)], name = name)\n",
    "    \n",
    "    cnn_model_4MSE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    cnn_model_4MSE.fit(train_images, label_update_4layerMSE, epochs=10)\n",
    "    probability_cnn_model_4MSE = tf.keras.Sequential([cnn_model_4MSE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_4CNNMSE[i], test_acc_4CNNMSE[i] = cnn_model_4MSE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_4CNNMSE[i])\n",
    "    label_update_4layerMSE = probability_cnn_model_4MSE.predict(train_images)\n",
    "    predictions_4layerMSE[:,:,i] = probability_cnn_model_4MSE.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the feed-forward NN with cross-entropy loss and mean-squared-error loss\n",
    "###### Training the 2-layer feed-forward NN with cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 1\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 2.0718 - accuracy: 0.4247\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.8619 - accuracy: 0.6224\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7797 - accuracy: 0.7139\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7432 - accuracy: 0.7413\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.7255 - accuracy: 0.7533\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7142 - accuracy: 0.7603\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7058 - accuracy: 0.7673\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7003 - accuracy: 0.7709\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6952 - accuracy: 0.7751\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6917 - accuracy: 0.7776\n",
      "10000/10000 - 1s - loss: 1.6957 - accuracy: 0.7736\n",
      "\n",
      "Test accuracy: 0.7736\n",
      "ROUND 2\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.2990 - accuracy: 0.2559\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2889 - accuracy: 0.4771\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2778 - accuracy: 0.5792\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2695 - accuracy: 0.6759\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2620 - accuracy: 0.7606\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2581 - accuracy: 0.7794\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2554 - accuracy: 0.8021\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2533 - accuracy: 0.8306\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2515 - accuracy: 0.8557\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2500 - accuracy: 0.8698\n",
      "10000/10000 - 1s - loss: 1.8368 - accuracy: 0.7203\n",
      "\n",
      "Test accuracy: 0.7203\n",
      "ROUND 3\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3005 - accuracy: 0.2329\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2928 - accuracy: 0.4693\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2826 - accuracy: 0.6669\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2742 - accuracy: 0.7566\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2699 - accuracy: 0.7972\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2680 - accuracy: 0.8132\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2671 - accuracy: 0.8225\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2663 - accuracy: 0.8310\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2658 - accuracy: 0.8409\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2654 - accuracy: 0.8496\n",
      "10000/10000 - 1s - loss: 1.8961 - accuracy: 0.6494\n",
      "\n",
      "Test accuracy: 0.6494\n",
      "ROUND 4\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 2.2990 - accuracy: 0.3067\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2914 - accuracy: 0.5223\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2826 - accuracy: 0.7790\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2765 - accuracy: 0.8438\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2738 - accuracy: 0.8662\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2726 - accuracy: 0.8785\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2720 - accuracy: 0.8861\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2716 - accuracy: 0.8932\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2713 - accuracy: 0.8995\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2712 - accuracy: 0.9021\n",
      "10000/10000 - 1s - loss: 1.9278 - accuracy: 0.6289\n",
      "\n",
      "Test accuracy: 0.6289\n",
      "ROUND 5\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.2992 - accuracy: 0.2699\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2912 - accuracy: 0.5019\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2849 - accuracy: 0.6330\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2797 - accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2761 - accuracy: 0.8797\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2745 - accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2737 - accuracy: 0.9024\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2733 - accuracy: 0.9066\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2731 - accuracy: 0.9124\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2730 - accuracy: 0.9154\n",
      "10000/10000 - 1s - loss: 1.9435 - accuracy: 0.6181\n",
      "\n",
      "Test accuracy: 0.6181\n",
      "ROUND 6\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.2986 - accuracy: 0.3235\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2914 - accuracy: 0.5485\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2850 - accuracy: 0.7433\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2802 - accuracy: 0.8154\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2772 - accuracy: 0.8735\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2758 - accuracy: 0.8953\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2751 - accuracy: 0.9063\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2747 - accuracy: 0.9112\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2745 - accuracy: 0.9148\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2744 - accuracy: 0.9204\n",
      "10000/10000 - 1s - loss: 1.9527 - accuracy: 0.6122\n",
      "\n",
      "Test accuracy: 0.6122\n",
      "ROUND 7\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3010 - accuracy: 0.2127\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.2942 - accuracy: 0.4949\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2879 - accuracy: 0.6719\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2826 - accuracy: 0.8073\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.2791 - accuracy: 0.8676\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2773 - accuracy: 0.8907\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2765 - accuracy: 0.9030\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.2761 - accuracy: 0.9081\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2758 - accuracy: 0.9133\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2757 - accuracy: 0.9192\n",
      "10000/10000 - 1s - loss: 1.9624 - accuracy: 0.6098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.6098\n",
      "ROUND 8\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 2.3010 - accuracy: 0.2901\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2964 - accuracy: 0.4954\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2901 - accuracy: 0.6557\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2834 - accuracy: 0.7917\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2797 - accuracy: 0.8666\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2781 - accuracy: 0.8894\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.2774 - accuracy: 0.9001\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2770 - accuracy: 0.9075\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2768 - accuracy: 0.9135\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.2767 - accuracy: 0.9172\n",
      "10000/10000 - 1s - loss: 1.9695 - accuracy: 0.6114\n",
      "\n",
      "Test accuracy: 0.6114\n"
     ]
    }
   ],
   "source": [
    "train_labels_Iterations_2layerFF_CE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_2layerFF_CE = train_labels_onehot\n",
    "predictions_2layerFF_CE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_2FFCE = np.zeros(8,dtype='float32')\n",
    "test_acc_2FFCE = np.zeros(8,dtype='float32')\n",
    "for i in range(8):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Feedforward_2_Layer_CE'\n",
    "    ff_model_2CE = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=im_shape),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10, activation ='softmax')], name = name)\n",
    "    \n",
    "    ff_model_2CE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ff_model_2CE.fit(train_images, label_update_2layerFF_CE, epochs=10)\n",
    "    probability_ff_model_2CE = tf.keras.Sequential([ff_model_2CE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_2FFCE[i], test_acc_2FFCE[i] = ff_model_2CE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_2FFCE[i])\n",
    "    label_update_2layerFF_CE = probability_ff_model_2CE.predict(train_images)\n",
    "    predictions_2layerFF_CE[:,:,i] = probability_ff_model_2CE.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training the 2-layer feed-forward NN with mean-squared-error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 1\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0860 - accuracy: 0.2855\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0723 - accuracy: 0.5178\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0577 - accuracy: 0.6094\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0493 - accuracy: 0.6521\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0449 - accuracy: 0.6781\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0421 - accuracy: 0.7042\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0398 - accuracy: 0.7301\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0379 - accuracy: 0.7512\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0362 - accuracy: 0.7651\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0349 - accuracy: 0.7726\n",
      "10000/10000 - 1s - loss: 0.0335 - accuracy: 0.7777\n",
      "\n",
      "Test accuracy: 0.0717\n",
      "ROUND 2\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0012 - accuracy: 0.1599\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 9.4564e-04 - accuracy: 0.2398\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 8.2874e-04 - accuracy: 0.3111\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 7.5755e-04 - accuracy: 0.3726\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 7.0454e-04 - accuracy: 0.4149\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 6.5896e-04 - accuracy: 0.4529\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 6.2157e-04 - accuracy: 0.4868\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 5.8978e-04 - accuracy: 0.5129\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 5.6306e-04 - accuracy: 0.5346\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 5.3969e-04 - accuracy: 0.5512\n",
      "10000/10000 - 1s - loss: 0.0841 - accuracy: 0.5434\n",
      "\n",
      "Test accuracy: 0.0717\n",
      "ROUND 3\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 6.7405e-04 - accuracy: 0.0719\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 3.3139e-04 - accuracy: 0.0804\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 2.7719e-04 - accuracy: 0.0869\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 2.4906e-04 - accuracy: 0.0925\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 2.2797e-04 - accuracy: 0.0985\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 2.1106e-04 - accuracy: 0.1032\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1.9795e-04 - accuracy: 0.1058\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1.8540e-04 - accuracy: 0.1092\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.7530e-04 - accuracy: 0.1111\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1.6647e-04 - accuracy: 0.1127\n",
      "10000/10000 - 1s - loss: 0.0899 - accuracy: 0.1262\n",
      "\n",
      "Test accuracy: 0.0717\n"
     ]
    }
   ],
   "source": [
    "train_labels_Iterations_2layerFF_MSE = np.zeros((60000,10,8),dtype='float32')\n",
    "label_update_2layerFF_MSE = train_labels_onehot\n",
    "predictions_2layerFF_MSE = np.zeros((10000,10,8),dtype='float32')\n",
    "test_loss_2FFMSE = np.zeros(8,dtype='float32')\n",
    "test_acc_2FFMSE = np.zeros(8,dtype='float32')\n",
    "for i in range(8):\n",
    "    print('ROUND',i+1)\n",
    "    \n",
    "    name = 'Feedforward_2_Layer_MSE'\n",
    "    ff_model_2MSE = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=im_shape),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(128,activation='relu'),\n",
    "    keras.layers.Dense(10, activation ='softmax')], name = name)\n",
    "    \n",
    "    ff_model_2MSE.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ff_model_2MSE.fit(train_images, label_update_2layerFF_MSE, epochs=10)\n",
    "    probability_ff_model_2MSE = tf.keras.Sequential([ff_model_2MSE, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "    test_loss_2FFMSE[i], test_acc_2FFMSE[i] = ff_model_2MSE.evaluate(test_images,  test_labels_onehot, verbose=2)\n",
    "    print('\\nTest accuracy:', test_acc_2FFMSE[i])\n",
    "    label_update_2layerFF_MSE = probability_ff_model_2MSE.predict(train_images)\n",
    "    predictions_2layerFF_MSE[:,:,i] = probability_ff_model_2MSE.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_acc_2layerFF_MSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-05ad73911592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc_2FFCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc_2layerFF_MSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss_2FFCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss_2layerFF_MSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_acc_2layerFF_MSE' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n",
    "\n",
    "ax1.plot(test_acc_2CNNCE,label='2 Layer CNN, CE')\n",
    "ax1.plot(test_acc_2CNNMSE,label='2 Layer CNN, MSE')\n",
    "ax1.plot(test_acc_4CNNCE,label='4 Layer CNN, CE')\n",
    "ax1.plot(test_acc_4CNNMSE,label='4 Layer CNN, CE')\n",
    "ax1.plot(test_acc_2FFCE,label='2 Layer FF, CE')\n",
    "ax1.plot(test_acc_2FFMSE,label='2 Layer FF, MSE')\n",
    "\n",
    "ax2.plot(test_loss_2CNNCE,label='2 Layer CNN, CE')\n",
    "ax2.plot(test_loss_2CNNMSE,label='2 Layer CNN, MSE')\n",
    "ax2.plot(test_loss_4CNNCE,label='4 Layer CNN, CE')\n",
    "ax2.plot(test_loss_4CNNMSE,label='4 Layer CNN, CE')\n",
    "ax2.plot(test_loss_2FFCE,label='2 Layer FF, CE')\n",
    "ax2.plot(test_loss_2FFMSE,label='2 Layer FF, MSE')\n",
    "\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('Self-Distillation steps')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
